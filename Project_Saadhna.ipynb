{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XySYBPJOSh9Q"
      },
      "outputs": [],
      "source": [
        "#Installing necessary libraries\n",
        "!pip install google-cloud-bigquery google-cloud-language requests gradio nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNm_KS4JU5iR"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth as google_auth\n",
        "google_auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJaYYzlAVFfF"
      },
      "outputs": [],
      "source": [
        "import vertexai\n",
        "PROJECT_ID = \"project-saadhna-430117\" #enter your project id here\n",
        "vertexai.init(project=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKuqsQ-KUVaT"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "from google.cloud import language_v1\n",
        "import requests\n",
        "import datetime\n",
        "\n",
        "client_bq = bigquery.Client(project='project-saadhna-430117')\n",
        "\n",
        "project_id = 'project-saadhna-430117'\n",
        "dataset_id = 'customer_reviews'\n",
        "table_id = 'google_reviews_new'\n",
        "\n",
        "# # Your Google Places API key\n",
        "google_places_api_key = '' #enter your google places api key here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWRTUjjeiS3N"
      },
      "outputs": [],
      "source": [
        "# # Define the table schema\n",
        "# schema = [\n",
        "#     bigquery.SchemaField('review_id', bigquery.enums.SqlTypeNames.STRING),\n",
        "#     bigquery.SchemaField('business_id', bigquery.enums.SqlTypeNames.STRING),\n",
        "#     bigquery.SchemaField('user_id', bigquery.enums.SqlTypeNames.STRING),\n",
        "#     bigquery.SchemaField('rating', bigquery.enums.SqlTypeNames.FLOAT),\n",
        "#     bigquery.SchemaField('review_text', bigquery.enums.SqlTypeNames.STRING),\n",
        "#     bigquery.SchemaField('created_at', bigquery.enums.SqlTypeNames.TIMESTAMP),\n",
        "#     bigquery.SchemaField('sentiment_score', bigquery.enums.SqlTypeNames.FLOAT),\n",
        "#     bigquery.SchemaField('sentiment_magnitude', bigquery.enums.SqlTypeNames.FLOAT),\n",
        "#     bigquery.SchemaField('sentiment', bigquery.enums.SqlTypeNames.STRING)\n",
        "# ]\n",
        "\n",
        "# # Create the table\n",
        "# table = bigquery.Table(f\"{project_id}.{dataset_id}.{table_id}\", schema=schema)\n",
        "# table = client_bq.create_table(table)\n",
        "# print(f\"Created table {table.table_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yvLDOkOUo__"
      },
      "outputs": [],
      "source": [
        "def get_place_id(place_name, location):\n",
        "    url = \"https://maps.googleapis.com/maps/api/place/findplacefromtext/json\"\n",
        "    params = {\n",
        "        'input': place_name,\n",
        "        'inputtype': 'textquery',\n",
        "        'fields': 'place_id',\n",
        "        'locationbias': f'point:{location}',  # optional, format: \"lat,lng\"\n",
        "        'key': google_places_api_key\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    candidates = response.json().get('candidates', [])\n",
        "    if candidates:\n",
        "        return candidates[0]['place_id']\n",
        "    return None\n",
        "\n",
        "def get_google_reviews(place_id):\n",
        "    url = f\"https://maps.googleapis.com/maps/api/place/details/json?place_id={place_id}&fields=reviews&key={google_places_api_key}\"\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Raise an exception if there's an HTTP error\n",
        "    reviews = response.json().get('result', {}).get('reviews', [])\n",
        "    return reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43CP2rJsu0s0"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.cloud import language_v1\n",
        "\n",
        "# Path to your service account key file\n",
        "service_account_key = '' #enter your service account key here\n",
        "\n",
        "# Initialize the Cloud Natural Language client\n",
        "client = language_v1.LanguageServiceClient.from_service_account_json(service_account_key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgLh7nQwSGtD"
      },
      "outputs": [],
      "source": [
        "from google.cloud import language_v1\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    \"\"\"Analyzes the sentiment of the given text.\"\"\"\n",
        "    client = language_v1.LanguageServiceClient()\n",
        "\n",
        "    document = language_v1.Document(\n",
        "        content=text, type_=language_v1.Document.Type.PLAIN_TEXT\n",
        "    )\n",
        "\n",
        "    sentiment = client.analyze_sentiment(request={\"document\": document}).document_sentiment\n",
        "\n",
        "    return sentiment.score, sentiment.magnitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JgPQLYmh-HOq"
      },
      "outputs": [],
      "source": [
        "# Inserting data into BigQuery table\n",
        "# from google.cloud import language_v1\n",
        "\n",
        "# # Path to your service account key file\n",
        "# service_account_key = ''  # Replace with the correct file name\n",
        "\n",
        "# # Initialize the Cloud Natural Language client\n",
        "# c = language_v1.LanguageServiceClient.from_service_account_json(service_account_key)\n",
        "\n",
        "# # Function to analyze sentiment\n",
        "# def analyze_sentiment(text):\n",
        "#     # Create a document with the text content\n",
        "#     document = language_v1.Document(\n",
        "#         content=text,\n",
        "#         type_=language_v1.Document.Type.PLAIN_TEXT\n",
        "#     )\n",
        "\n",
        "#     # Call the Cloud Natural Language API to analyze sentiment\n",
        "#     sentiment = c.analyze_sentiment(request={'document': document}).document_sentiment\n",
        "\n",
        "#     # Return sentiment score and magnitude\n",
        "#     return sentiment.score, sentiment.magnitude\n",
        "\n",
        "# def insert_reviews_to_bigquery(reviews, place_id, table_ref, p_name):\n",
        "#     rows_to_insert = []\n",
        "#     for review in reviews:\n",
        "#         created_at = datetime.datetime.fromtimestamp(review.get('time'))\n",
        "#         sentiment_score, sentiment_magnitude = analyze_sentiment(review.get('text'))\n",
        "\n",
        "#         row = {\n",
        "#             'review_id': f\"{place_id}_{review.get('author_name', '')}\",  # Unique ID\n",
        "#             'business_id': place_id,\n",
        "#             'user_id': review.get('author_name'),\n",
        "#             'rating': review.get('rating'),\n",
        "#             'review_text': review.get('text'),\n",
        "#             'created_at': created_at.isoformat(),\n",
        "#             'sentiment_score': sentiment_score,\n",
        "#             'sentiment_magnitude': sentiment_magnitude,\n",
        "#             'sentiment': 'positive' if sentiment_score > 0 else 'negative' if sentiment_score < 0 else 'neutral',\n",
        "#             'place_name': p_name\n",
        "#         }\n",
        "\n",
        "#         # Print each row to debug\n",
        "#         print(\"Row to insert:\", row)\n",
        "\n",
        "#         rows_to_insert.append(row)\n",
        "\n",
        "#     if client_bq.get_table(table_ref):  # API request\n",
        "#         errors = client_bq.insert_rows_json(table_ref, rows_to_insert)\n",
        "#         if errors:\n",
        "#             print(f'Errors occurred while inserting rows: {errors}')\n",
        "#         else:\n",
        "#             print('Inserted rows successfully.')\n",
        "#     else:\n",
        "#         print(f\"Table {table_ref.table_id} doesn't exist.\")\n",
        "\n",
        "# # Example usage\n",
        "# p_name = input(\"Enter a place: \")\n",
        "# location = ''  # New York City coordinates\n",
        "# place_id = get_place_id(p_name, location)\n",
        "\n",
        "# if place_id:\n",
        "#     print(f'Place ID for {p_name}: {place_id}')\n",
        "#     google_reviews = get_google_reviews(place_id)\n",
        "#     # Use the BigQuery client to get the table reference, including the project ID\n",
        "#     table_ref = client_bq.dataset(dataset_id).table(table_id)\n",
        "#     table_ref = client_bq.get_table(f\"{project_id}.{dataset_id}.{table_id}\")\n",
        "#     insert_reviews_to_bigquery(google_reviews, place_id, table_ref, p_name)\n",
        "# else:\n",
        "#     print(f'Place ID for {p_name} not found.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onINQAN-kJZi"
      },
      "outputs": [],
      "source": [
        "# Creating VertexAI dataset\n",
        "# import vertexai\n",
        "# from google.cloud import aiplatform\n",
        "\n",
        "# # Initialize the Vertex AI SDK\n",
        "# project_id = 'project-saadhna-430117'\n",
        "# region = 'us-central1'  # e.g., 'us-central1'\n",
        "# aiplatform.init(project=project_id, location=region)\n",
        "\n",
        "# # BigQuery table details\n",
        "# bq_table_uri = 'bq://project-saadhna-430117.customer_reviews.google_reviews_new'\n",
        "\n",
        "# # Create a Vertex AI Dataset\n",
        "# dataset = aiplatform.TabularDataset.create(\n",
        "#     display_name='customer_reviews_dataset',\n",
        "#     bq_source=bq_table_uri\n",
        "# )\n",
        "\n",
        "# print(f\"Created dataset: {dataset.resource_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1cJPVuqxQY1"
      },
      "outputs": [],
      "source": [
        "# Extracting Dataset resource name\n",
        "# from google.cloud import aiplatform\n",
        "\n",
        "# # Initialize the Vertex AI client\n",
        "# aiplatform.init(project='project-saadhna-430117', location='us-central1')\n",
        "\n",
        "# # List datasets\n",
        "# datasets = aiplatform.TabularDataset.list()\n",
        "\n",
        "# # Print dataset resource names\n",
        "# for dataset in datasets:\n",
        "#     print(dataset.resource_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-RUmMwjv9MJ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Training Model\n",
        "# from google.cloud import aiplatform\n",
        "\n",
        "# # Initialize the Vertex AI client\n",
        "# aiplatform.init(project='project-saadhna-430117', location='us-central1')\n",
        "\n",
        "# # Define the model training parameters\n",
        "# training_task_inputs = {\n",
        "#     \"targetColumn\": \"sentiment\",\n",
        "#     \"transformations\": [\n",
        "#         {\"numeric\": {\"column_name\": \"sentiment_score\"}},\n",
        "#         {\"numeric\": {\"column_name\": \"sentiment_magnitude\"}},\n",
        "#         {\"text\": {\"column_name\": \"review_text\"}}\n",
        "#     ],\n",
        "#     \"trainBudgetMilliNodeHours\": 1000,\n",
        "#     \"disableEarlyStopping\": False\n",
        "# }\n",
        "\n",
        "# # Create and run the AutoML training job\n",
        "# training_job = aiplatform.AutoMLTabularTrainingJob(\n",
        "#     display_name='automl_customer_reviews_sentiment',\n",
        "#     optimization_prediction_type='classification',\n",
        "#     optimization_objective='minimize-log-loss'\n",
        "# )\n",
        "\n",
        "# # Replace with your actual dataset object\n",
        "# dataset = aiplatform.TabularDataset('') # Replace with the actual Dataset object\n",
        "\n",
        "# # Run the training job\n",
        "# model = training_job.run(\n",
        "#     dataset=dataset, # Pass the Dataset object here\n",
        "#     model_display_name='customer_reviews_sentiment_model',\n",
        "#     training_fraction_split=0.8,\n",
        "#     validation_fraction_split=0.1,\n",
        "#     test_fraction_split=0.1,\n",
        "#     target_column='sentiment',\n",
        "#     budget_milli_node_hours=1000,\n",
        "#     disable_early_stopping=False\n",
        "# )\n",
        "\n",
        "# print(f\"Trained model: {model.resource_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Deploying Model to endpoint\n",
        "# from google.cloud import aiplatform\n",
        "\n",
        "# # Initialize the AI Platform client\n",
        "# aiplatform.init(project='project-saadhna-430117', location='us-central1')\n",
        "\n",
        "# # Deploy the trained model\n",
        "# endpoint = model.deploy(\n",
        "#     machine_type='n1-standard-4'  # Start with n1-standard-4\n",
        "# )\n",
        "\n",
        "# print(f\"Deployed model to endpoint: {endpoint.resource_name}\")"
      ],
      "metadata": {
        "id": "e4Zl--FO-tFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Model\n",
        "# from google.cloud import aiplatform\n",
        "# from datetime import datetime\n",
        "\n",
        "# # Initialize the Vertex AI client\n",
        "# aiplatform.init(project='project-saadhna-430117', location='us-central1')\n",
        "# # Specify the endpoint\n",
        "# endpoint_name = 'projects/546900673488/locations/us-central1/endpoints/9122074030587772928'\n",
        "\n",
        "# # Prepare the input data\n",
        "# input_data = {\n",
        "#     \"instances\": [\n",
        "#         {\n",
        "#             \"review_id\": \"ChIJr5YkQUJiwokRxjOrpYhD6u4_S S\",\n",
        "#             \"business_id\": \"ChIJr5YkQUJiwokRxjOrpYhD6u4\",\n",
        "#             \"user_id\": \"S S\",\n",
        "#             \"rating\": 2.0,\n",
        "#             \"review_text\": \"So this is my first time here and I order pani puri from them it's was good but it's not worth the price for 8 pieces and the p u r I balls was very small a least you can put big one so I don't think I will ever order p a n I p u r I from them or I won't be back as a customer just a rip off. F y I I...\",\n",
        "#             \"created_at\": \"2024-06-05 13:13:27UTC\",  # Assuming this is the actual format\n",
        "#             \"sentiment_score\": -0.40000000596046448,\n",
        "#             \"sentiment_magnitude\": 0.800000011920929,\n",
        "#             \"place_name\": \"Haldirams\"\n",
        "#         }\n",
        "#     ]\n",
        "# }\n",
        "\n",
        "# for instance in input_data['instances']:\n",
        "#     # Correct the time zone offset\n",
        "#     instance['created_at'] = instance['created_at'].replace(\"UTC\", \"+0000\")\n",
        "#     # Format the 'created_at' field\n",
        "#     datetime_object = datetime.strptime(instance['created_at'], '%Y-%m-%d %H:%M:%S%z')\n",
        "#     instance['created_at'] = datetime_object.strftime('%Y-%m-%d %H:%M:%S%z')\n",
        "\n",
        "# # Create a client for the model endpoint\n",
        "# endpoint = aiplatform.Endpoint(endpoint_name)\n",
        "\n",
        "# # Make predictions\n",
        "# response = endpoint.predict(instances=input_data['instances'])\n",
        "\n",
        "# # Print the prediction results\n",
        "# print(response)\n"
      ],
      "metadata": {
        "id": "niBEgergLp1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Ensure dependencies are installed\n",
        "os.system('pip install pandas google-cloud-bigquery google-cloud-language wordcloud matplotlib nltk gradio')\n",
        "\n",
        "# Download NLTK stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "print(\"All dependencies are installed and NLTK stopwords are downloaded.\")\n"
      ],
      "metadata": {
        "id": "jgfUMBqQiJtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !gcloud services list --enabled --project=project-saadhna-430117"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SMVFsmVJ5RtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth application-default login"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MLFV5-UOiekI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth application-default set-quota-project project-saadhna-430117"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GkOV230niytH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Integrating Customer Sentiment Analysis output with Gradio\n",
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "import gradio as gr\n",
        "import tempfile\n",
        "from google.cloud import language_v1\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# Initialize BigQuery client, explicitly providing the project ID\n",
        "client = bigquery.Client(project='project-saadhna-430117')\n",
        "\n",
        "# Initialize Google Cloud Natural Language client\n",
        "language_client = language_v1.LanguageServiceClient()\n",
        "\n",
        "# Function to fetch reviews from BigQuery\n",
        "def fetch_reviews(place_name):\n",
        "    query = f\"\"\"\n",
        "    SELECT review_text\n",
        "    FROM `project-saadhna-430117.customer_reviews.google_reviews_new`\n",
        "    WHERE LOWER(place_name) = '{place_name.lower()}'\n",
        "    \"\"\"\n",
        "    query_job = client.query(query)\n",
        "    df = query_job.to_dataframe()\n",
        "    return df\n",
        "\n",
        "# Function to preprocess the text\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "# Function to create a word cloud\n",
        "def create_word_cloud(text):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.png')\n",
        "    wordcloud.to_file(temp_file.name)\n",
        "    return temp_file.name\n",
        "\n",
        "# Function to analyze sentiment with retries\n",
        "def analyze_sentiment(text, retries=3):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            document = language_v1.Document(content=text, type_=language_v1.Document.Type.PLAIN_TEXT)\n",
        "            sentiment = language_client.analyze_sentiment(request={'document': document}).document_sentiment\n",
        "            return sentiment.score, sentiment.magnitude\n",
        "        except Exception as e:\n",
        "            if attempt < retries - 1:\n",
        "                print(f\"Error: {e}. Retrying...\")\n",
        "                time.sleep(5)\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "# Function to extract common keywords\n",
        "def extract_keywords(text, num_keywords=10):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # POS tagging\n",
        "    pos_tags = pos_tag(filtered_words)\n",
        "\n",
        "    # Define relevant POS tags\n",
        "    relevant_tags = {'NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'JJR', 'JJS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'}\n",
        "\n",
        "    # Filter words based on POS tags\n",
        "    relevant_words = [word for word, tag in pos_tags if tag in relevant_tags]\n",
        "\n",
        "    common_words = Counter(relevant_words).most_common(num_keywords)\n",
        "    return [word for word, count in common_words]\n",
        "\n",
        "# Function to generate actionable insights\n",
        "def generate_insights(df):\n",
        "    positive_reviews = df[df['sentiment_score'] > 0.25]['review_text']\n",
        "    negative_reviews = df[df['sentiment_score'] < -0.25]['review_text']\n",
        "\n",
        "    positive_keywords = extract_keywords(' '.join(positive_reviews))\n",
        "    negative_keywords = extract_keywords(' '.join(negative_reviews))\n",
        "\n",
        "    insights = []\n",
        "    if positive_keywords:\n",
        "        insights.append(f\"Strengths: Common positive aspects mentioned in reviews are {', '.join(positive_keywords)}.\")\n",
        "    if negative_keywords:\n",
        "        insights.append(f\"Weaknesses: Common complaints or issues mentioned in reviews are {', '.join(negative_keywords)}.\")\n",
        "\n",
        "    overall_sentiment_score = df['sentiment_score'].mean()\n",
        "    overall_sentiment_magnitude = df['sentiment_magnitude'].mean()\n",
        "\n",
        "    if overall_sentiment_score < -0.25:\n",
        "        insights.append(\"Consider addressing common complaints or issues mentioned in the reviews to improve customer satisfaction.\")\n",
        "    if overall_sentiment_score > 0.25:\n",
        "        insights.append(\"Positive sentiment detected! Continue leveraging your strengths to maintain high customer satisfaction.\")\n",
        "    if overall_sentiment_magnitude > 0.5:\n",
        "        insights.append(\"High sentiment magnitude indicates strong emotions in the reviews. Focus on areas with strong feedback.\")\n",
        "\n",
        "    return insights\n",
        "\n",
        "# Function to handle user input and generate results\n",
        "def generate_results(place_name):\n",
        "    try:\n",
        "        df = fetch_reviews(place_name)\n",
        "        if df.empty:\n",
        "            return \"No reviews found for this place.\", [], None, \"\"\n",
        "\n",
        "        # Sentiment analysis\n",
        "        df['sentiment_score'], df['sentiment_magnitude'] = zip(*df['review_text'].apply(analyze_sentiment))\n",
        "        all_text = ' '.join(df['review_text'].tolist())\n",
        "        insights = generate_insights(df)\n",
        "\n",
        "        # Word cloud\n",
        "        df['cleaned_text'] = df['review_text'].apply(preprocess_text)\n",
        "        all_cleaned_text = ' '.join(df['cleaned_text'].tolist())\n",
        "        word_cloud_image = create_word_cloud(all_cleaned_text)\n",
        "\n",
        "        # Construct the link to the Looker Studio dashboard with a message\n",
        "        dashboard_message = \"<p>Click the link below to view detailed visualizations and insights in the Looker Data Studio dashboard:</p>\"\n",
        "        dashboard_link = f\"\"\"\n",
        "        <a href=\"https://lookerstudio.google.com/reporting/8afc3f9a-b51b-445f-945b-8c692e573bc2?params=place_name={place_name}\" target=\"_blank\">\n",
        "            Open Dashboard\n",
        "        </a>\n",
        "        \"\"\"\n",
        "\n",
        "        # Return results\n",
        "        sentiment_summary = f\"Overall Sentiment Score: {df['sentiment_score'].mean()}, Overall Sentiment Magnitude: {df['sentiment_magnitude'].mean()}\"\n",
        "        return sentiment_summary, insights, word_cloud_image, f\"{dashboard_message}{dashboard_link}\", \"Created by Mihika Khemka\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return \"An error occurred while processing your request. Please ensure that the Cloud Natural Language API is enabled and configured correctly.\", [], None, \"\"\n",
        "\n",
        "# Create Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=generate_results,\n",
        "    inputs=gr.Textbox(lines=1, placeholder=\"Enter place name\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Sentiment Summary\"),\n",
        "        gr.Textbox(label=\"Insights\"),\n",
        "        gr.Image(type=\"filepath\", label=\"Word Cloud\"),  # Change type to \"filepath\"\n",
        "        gr.HTML(label=\"Data Studio Dashboard\"),\n",
        "        gr.HTML(label=\"Credits\")\n",
        "    ],\n",
        "    title=\"Customer Sentiment Analysis and Insights\",\n",
        "    description=\"Enter the place name to get sentiment analysis of customer reviews, actionable insights, and a word cloud.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio app\n",
        "iface.launch(debug=True)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fa0juPMP0Uzt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}