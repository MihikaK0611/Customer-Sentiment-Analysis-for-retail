{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XySYBPJOSh9Q",
        "outputId": "cc3706ff-793e-46cf-e885-9ea271c5d508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-bigquery in /usr/local/lib/python3.10/dist-packages (3.25.0)\n",
            "Requirement already satisfied: google-cloud-language in /usr/local/lib/python3.10/dist-packages (2.13.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Collecting gradio\n",
            "  Downloading gradio-4.39.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (2.19.1)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.27.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.7.1)\n",
            "Requirement already satisfied: packaging>=20.0.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (24.1)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.8.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-language) (1.24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-language) (3.20.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.111.1-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==1.1.1 (from gradio)\n",
            "  Downloading gradio_client-1.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.23.5)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.8.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.5.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.30.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.1.1->gradio) (2024.6.1)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==1.1.1->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.63.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery) (1.5.0)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.15.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery) (1.16.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting fastapi-cli>=0.0.2 (from fastapi->gradio)\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting email_validator>=2.0.0 (from fastapi->gradio)\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->gradio)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (0.6.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.12.0->fastapi->gradio)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0->fastapi->gradio)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0->fastapi->gradio)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0->fastapi->gradio)\n",
            "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-4.39.0-py3-none-any.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.1.1-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.2/318.2 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Downloading ruff-0.5.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading uvicorn-0.30.3-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.111.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=284953cc3a70286cfd402f93854a26ec7bd2e94077cea38b310cf7362fc00d73\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, uvloop, tomlkit, semantic-version, ruff, python-multipart, python-dotenv, orjson, httptools, h11, dnspython, aiofiles, watchfiles, uvicorn, starlette, httpcore, email_validator, httpx, gradio-client, fastapi-cli, fastapi, gradio\n",
            "  Attempting uninstall: tomlkit\n",
            "    Found existing installation: tomlkit 0.13.0\n",
            "    Uninstalling tomlkit-0.13.0:\n",
            "      Successfully uninstalled tomlkit-0.13.0\n",
            "Successfully installed aiofiles-23.2.1 dnspython-2.6.1 email_validator-2.2.0 fastapi-0.111.1 fastapi-cli-0.0.4 ffmpy-0.3.2 gradio-4.39.0 gradio-client-1.1.1 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 orjson-3.10.6 pydub-0.25.1 python-dotenv-1.0.1 python-multipart-0.0.9 ruff-0.5.5 semantic-version-2.10.0 starlette-0.37.2 tomlkit-0.12.0 uvicorn-0.30.3 uvloop-0.19.0 watchfiles-0.22.0 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "#Installing necessary libraries\n",
        "!pip install google-cloud-bigquery google-cloud-language requests gradio nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNm_KS4JU5iR"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth as google_auth\n",
        "google_auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJaYYzlAVFfF"
      },
      "outputs": [],
      "source": [
        "import vertexai\n",
        "PROJECT_ID = \"project-saadhna-430117\" #enter your project id here\n",
        "vertexai.init(project=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKuqsQ-KUVaT"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "from google.cloud import language_v1\n",
        "import requests\n",
        "import datetime\n",
        "\n",
        "client_bq = bigquery.Client(project='project-saadhna-430117')\n",
        "\n",
        "project_id = 'project-saadhna-430117'\n",
        "dataset_id = 'customer_reviews'\n",
        "table_id = 'google_reviews_new'\n",
        "\n",
        "# # Your Google Places API key\n",
        "google_places_api_key = '' #enter your google places api key here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWRTUjjeiS3N"
      },
      "outputs": [],
      "source": [
        "# # Define the table schema\n",
        "# schema = [\n",
        "#     bigquery.SchemaField('review_id', bigquery.enums.SqlTypeNames.STRING),\n",
        "#     bigquery.SchemaField('business_id', bigquery.enums.SqlTypeNames.STRING),\n",
        "#     bigquery.SchemaField('user_id', bigquery.enums.SqlTypeNames.STRING),\n",
        "#     bigquery.SchemaField('rating', bigquery.enums.SqlTypeNames.FLOAT),\n",
        "#     bigquery.SchemaField('review_text', bigquery.enums.SqlTypeNames.STRING),\n",
        "#     bigquery.SchemaField('created_at', bigquery.enums.SqlTypeNames.TIMESTAMP),\n",
        "#     bigquery.SchemaField('sentiment_score', bigquery.enums.SqlTypeNames.FLOAT),\n",
        "#     bigquery.SchemaField('sentiment_magnitude', bigquery.enums.SqlTypeNames.FLOAT),\n",
        "#     bigquery.SchemaField('sentiment', bigquery.enums.SqlTypeNames.STRING)\n",
        "# ]\n",
        "\n",
        "# # Create the table\n",
        "# table = bigquery.Table(f\"{project_id}.{dataset_id}.{table_id}\", schema=schema)\n",
        "# table = client_bq.create_table(table)\n",
        "# print(f\"Created table {table.table_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yvLDOkOUo__"
      },
      "outputs": [],
      "source": [
        "def get_place_id(place_name, location):\n",
        "    url = \"https://maps.googleapis.com/maps/api/place/findplacefromtext/json\"\n",
        "    params = {\n",
        "        'input': place_name,\n",
        "        'inputtype': 'textquery',\n",
        "        'fields': 'place_id',\n",
        "        'locationbias': f'point:{location}',  # optional, format: \"lat,lng\"\n",
        "        'key': google_places_api_key\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    candidates = response.json().get('candidates', [])\n",
        "    if candidates:\n",
        "        return candidates[0]['place_id']\n",
        "    return None\n",
        "\n",
        "def get_google_reviews(place_id):\n",
        "    url = f\"https://maps.googleapis.com/maps/api/place/details/json?place_id={place_id}&fields=reviews&key={google_places_api_key}\"\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Raise an exception if there's an HTTP error\n",
        "    reviews = response.json().get('result', {}).get('reviews', [])\n",
        "    return reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43CP2rJsu0s0"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.cloud import language_v1\n",
        "\n",
        "# Path to your service account key file\n",
        "service_account_key = '' #enter your service account key here\n",
        "\n",
        "# Initialize the Cloud Natural Language client\n",
        "client = language_v1.LanguageServiceClient.from_service_account_json(service_account_key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgLh7nQwSGtD"
      },
      "outputs": [],
      "source": [
        "from google.cloud import language_v1\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    \"\"\"Analyzes the sentiment of the given text.\"\"\"\n",
        "    client = language_v1.LanguageServiceClient()\n",
        "\n",
        "    document = language_v1.Document(\n",
        "        content=text, type_=language_v1.Document.Type.PLAIN_TEXT\n",
        "    )\n",
        "\n",
        "    sentiment = client.analyze_sentiment(request={\"document\": document}).document_sentiment\n",
        "\n",
        "    return sentiment.score, sentiment.magnitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JgPQLYmh-HOq"
      },
      "outputs": [],
      "source": [
        "# Inserting data into BigQuery table\n",
        "# from google.cloud import language_v1\n",
        "\n",
        "# # Path to your service account key file\n",
        "# service_account_key = ''  # Replace with the correct file name\n",
        "\n",
        "# # Initialize the Cloud Natural Language client\n",
        "# c = language_v1.LanguageServiceClient.from_service_account_json(service_account_key)\n",
        "\n",
        "# # Function to analyze sentiment\n",
        "# def analyze_sentiment(text):\n",
        "#     # Create a document with the text content\n",
        "#     document = language_v1.Document(\n",
        "#         content=text,\n",
        "#         type_=language_v1.Document.Type.PLAIN_TEXT\n",
        "#     )\n",
        "\n",
        "#     # Call the Cloud Natural Language API to analyze sentiment\n",
        "#     sentiment = c.analyze_sentiment(request={'document': document}).document_sentiment\n",
        "\n",
        "#     # Return sentiment score and magnitude\n",
        "#     return sentiment.score, sentiment.magnitude\n",
        "\n",
        "# def insert_reviews_to_bigquery(reviews, place_id, table_ref, p_name):\n",
        "#     rows_to_insert = []\n",
        "#     for review in reviews:\n",
        "#         created_at = datetime.datetime.fromtimestamp(review.get('time'))\n",
        "#         sentiment_score, sentiment_magnitude = analyze_sentiment(review.get('text'))\n",
        "\n",
        "#         row = {\n",
        "#             'review_id': f\"{place_id}_{review.get('author_name', '')}\",  # Unique ID\n",
        "#             'business_id': place_id,\n",
        "#             'user_id': review.get('author_name'),\n",
        "#             'rating': review.get('rating'),\n",
        "#             'review_text': review.get('text'),\n",
        "#             'created_at': created_at.isoformat(),\n",
        "#             'sentiment_score': sentiment_score,\n",
        "#             'sentiment_magnitude': sentiment_magnitude,\n",
        "#             'sentiment': 'positive' if sentiment_score > 0 else 'negative' if sentiment_score < 0 else 'neutral',\n",
        "#             'place_name': p_name\n",
        "#         }\n",
        "\n",
        "#         # Print each row to debug\n",
        "#         print(\"Row to insert:\", row)\n",
        "\n",
        "#         rows_to_insert.append(row)\n",
        "\n",
        "#     if client_bq.get_table(table_ref):  # API request\n",
        "#         errors = client_bq.insert_rows_json(table_ref, rows_to_insert)\n",
        "#         if errors:\n",
        "#             print(f'Errors occurred while inserting rows: {errors}')\n",
        "#         else:\n",
        "#             print('Inserted rows successfully.')\n",
        "#     else:\n",
        "#         print(f\"Table {table_ref.table_id} doesn't exist.\")\n",
        "\n",
        "# # Example usage\n",
        "# p_name = input(\"Enter a place: \")\n",
        "# location = ''  # New York City coordinates\n",
        "# place_id = get_place_id(p_name, location)\n",
        "\n",
        "# if place_id:\n",
        "#     print(f'Place ID for {p_name}: {place_id}')\n",
        "#     google_reviews = get_google_reviews(place_id)\n",
        "#     # Use the BigQuery client to get the table reference, including the project ID\n",
        "#     table_ref = client_bq.dataset(dataset_id).table(table_id)\n",
        "#     table_ref = client_bq.get_table(f\"{project_id}.{dataset_id}.{table_id}\")\n",
        "#     insert_reviews_to_bigquery(google_reviews, place_id, table_ref, p_name)\n",
        "# else:\n",
        "#     print(f'Place ID for {p_name} not found.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onINQAN-kJZi"
      },
      "outputs": [],
      "source": [
        "# Creating VertexAI dataset\n",
        "# import vertexai\n",
        "# from google.cloud import aiplatform\n",
        "\n",
        "# # Initialize the Vertex AI SDK\n",
        "# project_id = 'project-saadhna-430117'\n",
        "# region = 'us-central1'  # e.g., 'us-central1'\n",
        "# aiplatform.init(project=project_id, location=region)\n",
        "\n",
        "# # BigQuery table details\n",
        "# bq_table_uri = 'bq://project-saadhna-430117.customer_reviews.google_reviews_new'\n",
        "\n",
        "# # Create a Vertex AI Dataset\n",
        "# dataset = aiplatform.TabularDataset.create(\n",
        "#     display_name='customer_reviews_dataset',\n",
        "#     bq_source=bq_table_uri\n",
        "# )\n",
        "\n",
        "# print(f\"Created dataset: {dataset.resource_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1cJPVuqxQY1"
      },
      "outputs": [],
      "source": [
        "# Extracting Dataset resource name\n",
        "# from google.cloud import aiplatform\n",
        "\n",
        "# # Initialize the Vertex AI client\n",
        "# aiplatform.init(project='project-saadhna-430117', location='us-central1')\n",
        "\n",
        "# # List datasets\n",
        "# datasets = aiplatform.TabularDataset.list()\n",
        "\n",
        "# # Print dataset resource names\n",
        "# for dataset in datasets:\n",
        "#     print(dataset.resource_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-RUmMwjv9MJ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Training Model\n",
        "# from google.cloud import aiplatform\n",
        "\n",
        "# # Initialize the Vertex AI client\n",
        "# aiplatform.init(project='project-saadhna-430117', location='us-central1')\n",
        "\n",
        "# # Define the model training parameters\n",
        "# training_task_inputs = {\n",
        "#     \"targetColumn\": \"sentiment\",\n",
        "#     \"transformations\": [\n",
        "#         {\"numeric\": {\"column_name\": \"sentiment_score\"}},\n",
        "#         {\"numeric\": {\"column_name\": \"sentiment_magnitude\"}},\n",
        "#         {\"text\": {\"column_name\": \"review_text\"}}\n",
        "#     ],\n",
        "#     \"trainBudgetMilliNodeHours\": 1000,\n",
        "#     \"disableEarlyStopping\": False\n",
        "# }\n",
        "\n",
        "# # Create and run the AutoML training job\n",
        "# training_job = aiplatform.AutoMLTabularTrainingJob(\n",
        "#     display_name='automl_customer_reviews_sentiment',\n",
        "#     optimization_prediction_type='classification',\n",
        "#     optimization_objective='minimize-log-loss'\n",
        "# )\n",
        "\n",
        "# # Replace with your actual dataset object\n",
        "# dataset = aiplatform.TabularDataset('') # Replace with the actual Dataset object\n",
        "\n",
        "# # Run the training job\n",
        "# model = training_job.run(\n",
        "#     dataset=dataset, # Pass the Dataset object here\n",
        "#     model_display_name='customer_reviews_sentiment_model',\n",
        "#     training_fraction_split=0.8,\n",
        "#     validation_fraction_split=0.1,\n",
        "#     test_fraction_split=0.1,\n",
        "#     target_column='sentiment',\n",
        "#     budget_milli_node_hours=1000,\n",
        "#     disable_early_stopping=False\n",
        "# )\n",
        "\n",
        "# print(f\"Trained model: {model.resource_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Deploying Model to endpoint\n",
        "# from google.cloud import aiplatform\n",
        "\n",
        "# # Initialize the AI Platform client\n",
        "# aiplatform.init(project='project-saadhna-430117', location='us-central1')\n",
        "\n",
        "# # Deploy the trained model\n",
        "# endpoint = model.deploy(\n",
        "#     machine_type='n1-standard-4'  # Start with n1-standard-4\n",
        "# )\n",
        "\n",
        "# print(f\"Deployed model to endpoint: {endpoint.resource_name}\")"
      ],
      "metadata": {
        "id": "e4Zl--FO-tFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Model\n",
        "# from google.cloud import aiplatform\n",
        "# from datetime import datetime\n",
        "\n",
        "# # Initialize the Vertex AI client\n",
        "# aiplatform.init(project='project-saadhna-430117', location='us-central1')\n",
        "# # Specify the endpoint\n",
        "# endpoint_name = 'projects/546900673488/locations/us-central1/endpoints/9122074030587772928'\n",
        "\n",
        "# # Prepare the input data\n",
        "# input_data = {\n",
        "#     \"instances\": [\n",
        "#         {\n",
        "#             \"review_id\": \"ChIJr5YkQUJiwokRxjOrpYhD6u4_S S\",\n",
        "#             \"business_id\": \"ChIJr5YkQUJiwokRxjOrpYhD6u4\",\n",
        "#             \"user_id\": \"S S\",\n",
        "#             \"rating\": 2.0,\n",
        "#             \"review_text\": \"So this is my first time here and I order pani puri from them it's was good but it's not worth the price for 8 pieces and the p u r I balls was very small a least you can put big one so I don't think I will ever order p a n I p u r I from them or I won't be back as a customer just a rip off. F y I I...\",\n",
        "#             \"created_at\": \"2024-06-05 13:13:27UTC\",  # Assuming this is the actual format\n",
        "#             \"sentiment_score\": -0.40000000596046448,\n",
        "#             \"sentiment_magnitude\": 0.800000011920929,\n",
        "#             \"place_name\": \"Haldirams\"\n",
        "#         }\n",
        "#     ]\n",
        "# }\n",
        "\n",
        "# for instance in input_data['instances']:\n",
        "#     # Correct the time zone offset\n",
        "#     instance['created_at'] = instance['created_at'].replace(\"UTC\", \"+0000\")\n",
        "#     # Format the 'created_at' field\n",
        "#     datetime_object = datetime.strptime(instance['created_at'], '%Y-%m-%d %H:%M:%S%z')\n",
        "#     instance['created_at'] = datetime_object.strftime('%Y-%m-%d %H:%M:%S%z')\n",
        "\n",
        "# # Create a client for the model endpoint\n",
        "# endpoint = aiplatform.Endpoint(endpoint_name)\n",
        "\n",
        "# # Make predictions\n",
        "# response = endpoint.predict(instances=input_data['instances'])\n",
        "\n",
        "# # Print the prediction results\n",
        "# print(response)\n"
      ],
      "metadata": {
        "id": "niBEgergLp1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Ensure dependencies are installed\n",
        "os.system('pip install pandas google-cloud-bigquery google-cloud-language wordcloud matplotlib nltk gradio')\n",
        "\n",
        "# Download NLTK stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "print(\"All dependencies are installed and NLTK stopwords are downloaded.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgfUMBqQiJtM",
        "outputId": "84100892-675f-4229-efea-78857f13189b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All dependencies are installed and NLTK stopwords are downloaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !gcloud services list --enabled --project=project-saadhna-430117"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SMVFsmVJ5RtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth application-default login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MLFV5-UOiekI",
        "outputId": "af39b2e5-14f1-4bef-b04a-73bab2c784c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are running on a Google Compute Engine virtual machine.\n",
            "The service credentials associated with this virtual machine\n",
            "will automatically be used by Application Default\n",
            "Credentials, so it is not necessary to use this command.\n",
            "\n",
            "If you decide to proceed anyway, your user credentials may be visible\n",
            "to others with access to this virtual machine. Are you sure you want\n",
            "to authenticate with your personal account?\n",
            "\n",
            "Do you want to continue (Y/n)?  Y\n",
            "\n",
            "Go to the following link in your browser, and complete the sign-in prompts:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fapplicationdefaultauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=NQjBRv2mQFsIqUf6w8VXf4cxj6JRwA&prompt=consent&token_usage=remote&access_type=offline&code_challenge=Evpa2qPaNTjuWHnvdrUAOYqM_Z2RWNUcwqqEWWpaUXQ&code_challenge_method=S256\n",
            "\n",
            "Once finished, enter the verification code provided in your browser: 4/0AcvDMrCFd9Rh9mltonarJBCIeuWh7rxymZlVK_VH0sNF8xeWazvW6Ks4ogK9u0VuVRjvfg\n",
            "\n",
            "Credentials saved to file: [/content/.config/application_default_credentials.json]\n",
            "\n",
            "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
            "\u001b[1;33mWARNING:\u001b[0m \n",
            "Cannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth application-default set-quota-project project-saadhna-430117"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GkOV230niytH",
        "outputId": "43927f21-ff0c-4d4d-e2d8-54e79018201f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Credentials saved to file: [/content/.config/application_default_credentials.json]\n",
            "\n",
            "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
            "\n",
            "Quota project \"project-saadhna-430117\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Integrating Customer Sentiment Analysis output with Gradio\n",
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "import gradio as gr\n",
        "import tempfile\n",
        "from google.cloud import language_v1\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# Initialize BigQuery client, explicitly providing the project ID\n",
        "client = bigquery.Client(project='project-saadhna-430117')\n",
        "\n",
        "# Initialize Google Cloud Natural Language client\n",
        "language_client = language_v1.LanguageServiceClient()\n",
        "\n",
        "# Function to fetch reviews from BigQuery\n",
        "def fetch_reviews(place_name):\n",
        "    query = f\"\"\"\n",
        "    SELECT review_text\n",
        "    FROM `project-saadhna-430117.customer_reviews.google_reviews_new`\n",
        "    WHERE LOWER(place_name) = '{place_name.lower()}'\n",
        "    \"\"\"\n",
        "    query_job = client.query(query)\n",
        "    df = query_job.to_dataframe()\n",
        "    return df\n",
        "\n",
        "# Function to preprocess the text\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "# Function to create a word cloud\n",
        "def create_word_cloud(text):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.png')\n",
        "    wordcloud.to_file(temp_file.name)\n",
        "    return temp_file.name\n",
        "\n",
        "# Function to analyze sentiment with retries\n",
        "def analyze_sentiment(text, retries=3):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            document = language_v1.Document(content=text, type_=language_v1.Document.Type.PLAIN_TEXT)\n",
        "            sentiment = language_client.analyze_sentiment(request={'document': document}).document_sentiment\n",
        "            return sentiment.score, sentiment.magnitude\n",
        "        except Exception as e:\n",
        "            if attempt < retries - 1:\n",
        "                print(f\"Error: {e}. Retrying...\")\n",
        "                time.sleep(5)\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "# Function to extract common keywords\n",
        "def extract_keywords(text, num_keywords=10):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # POS tagging\n",
        "    pos_tags = pos_tag(filtered_words)\n",
        "\n",
        "    # Define relevant POS tags\n",
        "    relevant_tags = {'NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'JJR', 'JJS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'}\n",
        "\n",
        "    # Filter words based on POS tags\n",
        "    relevant_words = [word for word, tag in pos_tags if tag in relevant_tags]\n",
        "\n",
        "    common_words = Counter(relevant_words).most_common(num_keywords)\n",
        "    return [word for word, count in common_words]\n",
        "\n",
        "# Function to generate actionable insights\n",
        "def generate_insights(df):\n",
        "    positive_reviews = df[df['sentiment_score'] > 0.25]['review_text']\n",
        "    negative_reviews = df[df['sentiment_score'] < -0.25]['review_text']\n",
        "\n",
        "    positive_keywords = extract_keywords(' '.join(positive_reviews))\n",
        "    negative_keywords = extract_keywords(' '.join(negative_reviews))\n",
        "\n",
        "    insights = []\n",
        "    if positive_keywords:\n",
        "        insights.append(f\"Strengths: Common positive aspects mentioned in reviews are {', '.join(positive_keywords)}.\")\n",
        "    if negative_keywords:\n",
        "        insights.append(f\"Weaknesses: Common complaints or issues mentioned in reviews are {', '.join(negative_keywords)}.\")\n",
        "\n",
        "    overall_sentiment_score = df['sentiment_score'].mean()\n",
        "    overall_sentiment_magnitude = df['sentiment_magnitude'].mean()\n",
        "\n",
        "    if overall_sentiment_score < -0.25:\n",
        "        insights.append(\"Consider addressing common complaints or issues mentioned in the reviews to improve customer satisfaction.\")\n",
        "    if overall_sentiment_score > 0.25:\n",
        "        insights.append(\"Positive sentiment detected! Continue leveraging your strengths to maintain high customer satisfaction.\")\n",
        "    if overall_sentiment_magnitude > 0.5:\n",
        "        insights.append(\"High sentiment magnitude indicates strong emotions in the reviews. Focus on areas with strong feedback.\")\n",
        "\n",
        "    return insights\n",
        "\n",
        "# Function to handle user input and generate results\n",
        "def generate_results(place_name):\n",
        "    try:\n",
        "        df = fetch_reviews(place_name)\n",
        "        if df.empty:\n",
        "            return \"No reviews found for this place.\", [], None, \"\"\n",
        "\n",
        "        # Sentiment analysis\n",
        "        df['sentiment_score'], df['sentiment_magnitude'] = zip(*df['review_text'].apply(analyze_sentiment))\n",
        "        all_text = ' '.join(df['review_text'].tolist())\n",
        "        insights = generate_insights(df)\n",
        "\n",
        "        # Word cloud\n",
        "        df['cleaned_text'] = df['review_text'].apply(preprocess_text)\n",
        "        all_cleaned_text = ' '.join(df['cleaned_text'].tolist())\n",
        "        word_cloud_image = create_word_cloud(all_cleaned_text)\n",
        "\n",
        "        # Construct the link to the Looker Studio dashboard with a message\n",
        "        dashboard_message = \"<p>Click the link below to view detailed visualizations and insights in the Looker Data Studio dashboard:</p>\"\n",
        "        dashboard_link = f\"\"\"\n",
        "        <a href=\"https://lookerstudio.google.com/reporting/8afc3f9a-b51b-445f-945b-8c692e573bc2?params=place_name={place_name}\" target=\"_blank\">\n",
        "            Open Dashboard\n",
        "        </a>\n",
        "        \"\"\"\n",
        "\n",
        "        # Return results\n",
        "        sentiment_summary = f\"Overall Sentiment Score: {df['sentiment_score'].mean()}, Overall Sentiment Magnitude: {df['sentiment_magnitude'].mean()}\"\n",
        "        return sentiment_summary, insights, word_cloud_image, f\"{dashboard_message}{dashboard_link}\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return \"An error occurred while processing your request. Please ensure that the Cloud Natural Language API is enabled and configured correctly.\", [], None, \"\"\n",
        "\n",
        "# Create Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=generate_results,\n",
        "    inputs=gr.Textbox(lines=1, placeholder=\"Enter place name\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Sentiment Summary\"),\n",
        "        gr.Textbox(label=\"Insights\"),\n",
        "        gr.Image(type=\"filepath\", label=\"Word Cloud\"),  # Change type to \"filepath\"\n",
        "        gr.HTML(label=\"Data Studio Dashboard\")\n",
        "    ],\n",
        "    title=\"Customer Sentiment Analysis and Insights\",\n",
        "    description=\"Enter the place name to get sentiment analysis of customer reviews, actionable insights, and a word cloud.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio app\n",
        "iface.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "collapsed": true,
        "id": "fa0juPMP0Uzt",
        "outputId": "fc6e845c-01ab-455a-b713-29da45c520c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://c07c7a3a2fe5a481df.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://c07c7a3a2fe5a481df.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://c07c7a3a2fe5a481df.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}